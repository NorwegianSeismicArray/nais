<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>nais.Layers.attention API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nais.Layers.attention</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow.keras.layers as tfl 
import tensorflow as tf 
import tensorflow.keras.backend as K 


class SeqSelfAttention(tfl.Layer):
    ATTENTION_TYPE_ADD = &#39;additive&#39;
    ATTENTION_TYPE_MUL = &#39;multiplicative&#39;

    def __init__(self,
                 units=32,
                 attention_width=None,
                 attention_type=ATTENTION_TYPE_ADD,
                 return_attention=False,
                 history_only=False,
                 kernel_initializer=&#39;glorot_normal&#39;,
                 bias_initializer=&#39;zeros&#39;,
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 use_additive_bias=True,
                 use_attention_bias=True,
                 attention_activation=None,
                 attention_regularizer_weight=0.0,
                 **kwargs):
        
        &#34;&#34;&#34;Layer initialization. modified from https://github.com/CyberZHG
        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf
        Args
            units: The dimension of the vectors that used to calculate the attention weights.
            attention_width: The width of local attention.
            attention_type: &#39;additive&#39; or &#39;multiplicative&#39;.
            return_attention: Whether to return the attention weights for visualization.
            history_only: Only use historical pieces of data.
            kernel_initializer: The initializer for weight matrices.
            bias_initializer: The initializer for biases.
            kernel_regularizer: The regularization for weight matrices.
            bias_regularizer: The regularization for biases.
            kernel_constraint: The constraint for weight matrices.
            bias_constraint: The constraint for biases.
            use_additive_bias: Whether to use bias while calculating the relevance of inputs features
                                    in additive mode.
            use_attention_bias: Whether to use bias while calculating the weights of attention.
            attention_activation: The activation used for calculating the weights of attention.
            attention_regularizer_weight: The weights of attention regularizer.
            kwargs: Parameters for parent class.
        &#34;&#34;&#34; 

        super().__init__(**kwargs)
        self.supports_masking = True
        self.units = units
        self.attention_width = attention_width
        self.attention_type = attention_type
        self.return_attention = return_attention
        self.history_only = history_only
        if history_only and attention_width is None:
            self.attention_width = int(1e9)

        self.use_additive_bias = use_additive_bias
        self.use_attention_bias = use_attention_bias
        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)
        self.bias_initializer = tf.keras.initializers.get(bias_initializer)
        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)
        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)
        self.bias_constraint = tf.keras.constraints.get(bias_constraint)
        self.attention_activation = tf.keras.activations.get(attention_activation)
        self.attention_regularizer_weight = attention_regularizer_weight
        self._backend = tf.keras.backend.backend()

        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            self.Wx, self.Wt, self.bh = None, None, None
            self.Wa, self.ba = None, None
        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            self.Wa, self.ba = None, None
        else:
            raise NotImplementedError(&#39;No implementation for attention type : &#39; + attention_type)

    def get_config(self):
        config = {
            &#39;units&#39;: self.units,
            &#39;attention_width&#39;: self.attention_width,
            &#39;attention_type&#39;: self.attention_type,
            &#39;return_attention&#39;: self.return_attention,
            &#39;history_only&#39;: self.history_only,
            &#39;use_additive_bias&#39;: self.use_additive_bias,
            &#39;use_attention_bias&#39;: self.use_attention_bias,
            &#39;kernel_initializer&#39;: tf.keras.regularizers.serialize(self.kernel_initializer),
            &#39;bias_initializer&#39;: tf.keras.regularizers.serialize(self.bias_initializer),
            &#39;kernel_regularizer&#39;: tf.keras.regularizers.serialize(self.kernel_regularizer),
            &#39;bias_regularizer&#39;: tf.keras.regularizers.serialize(self.bias_regularizer),
            &#39;kernel_constraint&#39;: tf.keras.constraints.serialize(self.kernel_constraint),
            &#39;bias_constraint&#39;: tf.keras.constraints.serialize(self.bias_constraint),
            &#39;attention_activation&#39;: tf.keras.activations.serialize(self.attention_activation),
            &#39;attention_regularizer_weight&#39;: self.attention_regularizer_weight,
        }
        base_config = super(SeqSelfAttention, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def build(self, input_shape):
        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            self._build_additive_attention(input_shape)
        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            self._build_multiplicative_attention(input_shape)
        super(SeqSelfAttention, self).build(input_shape)

    def _build_additive_attention(self, input_shape):
        feature_dim = int(input_shape[2])

        self.Wt = self.add_weight(shape=(feature_dim, self.units),
                                  name=&#39;{}_Add_Wt&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        self.Wx = self.add_weight(shape=(feature_dim, self.units),
                                  name=&#39;{}_Add_Wx&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_additive_bias:
            self.bh = self.add_weight(shape=(self.units,),
                                      name=&#39;{}_Add_bh&#39;.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

        self.Wa = self.add_weight(shape=(self.units, 1),
                                  name=&#39;{}_Add_Wa&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_attention_bias:
            self.ba = self.add_weight(shape=(1,),
                                      name=&#39;{}_Add_ba&#39;.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

    def _build_multiplicative_attention(self, input_shape):
        feature_dim = int(input_shape[2])

        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),
                                  name=&#39;{}_Mul_Wa&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_attention_bias:
            self.ba = self.add_weight(shape=(1,),
                                      name=&#39;{}_Mul_ba&#39;.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

    def call(self, inputs, mask=None, **kwargs):
        input_len = K.shape(inputs)[1]

        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            e = self._call_additive_emission(inputs)
        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            e = self._call_multiplicative_emission(inputs)

        if self.attention_activation is not None:
            e = self.attention_activation(e)
        e = K.exp(e - K.max(e, axis=-1, keepdims=True))
        if self.attention_width is not None:
            if self.history_only:
                lower = K.arange(0, input_len) - (self.attention_width - 1)
            else:
                lower = K.arange(0, input_len) - self.attention_width // 2
            lower = K.expand_dims(lower, axis=-1)
            upper = lower + self.attention_width
            indices = K.expand_dims(K.arange(0, input_len), axis=0)
            e = e * K.cast(lower &lt;= indices, K.floatx()) * K.cast(indices &lt; upper, K.floatx())
        if mask is not None:
            mask = K.cast(mask, K.floatx())
            mask = K.expand_dims(mask)
            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))

        # a_{t} = \text{softmax}(e_t)
        s = K.sum(e, axis=-1, keepdims=True)
        a = e / (s + K.epsilon())

        # l_t = \sum_{t&#39;} a_{t, t&#39;} x_{t&#39;}
        v = K.batch_dot(a, inputs)
        if self.attention_regularizer_weight &gt; 0.0:
            self.add_loss(self._attention_regularizer(a))

        if self.return_attention:
            return [v, a]
        return v

    def _call_additive_emission(self, inputs):
        input_shape = K.shape(inputs)
        batch_size = input_shape[0]
        input_len = inputs.get_shape().as_list()[1]

        # h_{t, t&#39;} = \tanh(x_t^T W_t + x_{t&#39;}^T W_x + b_h)
        q = K.expand_dims(K.dot(inputs, self.Wt), 2)
        k = K.expand_dims(K.dot(inputs, self.Wx), 1)
        if self.use_additive_bias:
            h = K.tanh(q + k + self.bh)
        else:
            h = K.tanh(q + k)

        # e_{t, t&#39;} = W_a h_{t, t&#39;} + b_a
        if self.use_attention_bias:
            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))
        else:
            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))
        return e

    def _call_multiplicative_emission(self, inputs):
        # e_{t, t&#39;} = x_t^T W_a x_{t&#39;} + b_a
        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))
        if self.use_attention_bias:
            e += self.ba[0]
        return e

    def compute_output_shape(self, input_shape):
        output_shape = input_shape
        if self.return_attention:
            attention_shape = (input_shape[0], output_shape[1], input_shape[1])
            return [output_shape, attention_shape]
        return output_shape

    def compute_mask(self, inputs, mask=None):
        if self.return_attention:
            return [mask, None]
        return mask

    def _attention_regularizer(self, attention):
        batch_size = K.cast(K.shape(attention)[0], K.floatx())
        input_len = K.shape(attention)[-1]
        indices = K.expand_dims(K.arange(0, input_len), axis=0)
        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)
        eye = K.cast(K.equal(indices, diagonal), K.floatx())
        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(
            attention,
            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size

    @staticmethod
    def get_custom_objects():
        return {&#39;SeqSelfAttention&#39;: SeqSelfAttention}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="nais.Layers.attention.SeqSelfAttention"><code class="flex name class">
<span>class <span class="ident">SeqSelfAttention</span></span>
<span>(</span><span>units=32, attention_width=None, attention_type='additive', return_attention=False, history_only=False, kernel_initializer='glorot_normal', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_additive_bias=True, use_attention_bias=True, attention_activation=None, attention_regularizer_weight=0.0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainable</code></strong></dt>
<dd>Boolean, whether the layer's variables should be trainable.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>String name of the layer.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. Can also be a
<code>tf.keras.mixed_precision.Policy</code>, which allows the computation and weight
dtype to differ. Default of <code>None</code> means to use
<code>tf.keras.mixed_precision.global_policy()</code>, which is a float32 policy
unless set to different value.</dd>
<dt><strong><code>dynamic</code></strong></dt>
<dd>Set this to <code>True</code> if your layer should only be run eagerly, and
should not be used to generate a static computation graph.
This would be the case for a Tree-RNN or a recursive network,
for example, or generally for any layer that manipulates tensors
using Python control flow. If <code>False</code>, we assume that the layer can
safely be used to generate a static computation graph.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's weights.</dd>
<dt><strong><code>variable_dtype</code></strong></dt>
<dd>Alias of <code>dtype</code>.</dd>
<dt><strong><code>compute_dtype</code></strong></dt>
<dd>The dtype of the layer's computations. Layers automatically
cast inputs to this dtype which causes the computations and output to also
be in this dtype. When mixed precision is used with a
<code>tf.keras.mixed_precision.Policy</code>, this will be different than
<code>variable_dtype</code>.</dd>
<dt><strong><code>dtype_policy</code></strong></dt>
<dd>The layer's dtype policy. See the
<code>tf.keras.mixed_precision.Policy</code> documentation for details.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e. whether
its potentially-trainable weights should be returned as part of
<code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, inputs, *args, **kwargs)</code>: Called in <code>__call__</code> after making
sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in inference mode or training
mode). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method">the layer/model subclassing guide</a></li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method">the layer/model subclassing guide</a>
A typical signature for this method is <code>call(self, inputs)</code>, and user could
optionally add <code>training</code> and <code>mask</code> if the layer need them. <code>*args</code> and
<code>**kwargs</code> is only useful for future extension when more input parameters
are planned to be added.</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a></p>
<p>Layer initialization. modified from <a href="https://github.com/CyberZHG">https://github.com/CyberZHG</a>
For additive attention, see: <a href="https://arxiv.org/pdf/1806.01264.pdf">https://arxiv.org/pdf/1806.01264.pdf</a>
Args
units: The dimension of the vectors that used to calculate the attention weights.
attention_width: The width of local attention.
attention_type: 'additive' or 'multiplicative'.
return_attention: Whether to return the attention weights for visualization.
history_only: Only use historical pieces of data.
kernel_initializer: The initializer for weight matrices.
bias_initializer: The initializer for biases.
kernel_regularizer: The regularization for weight matrices.
bias_regularizer: The regularization for biases.
kernel_constraint: The constraint for weight matrices.
bias_constraint: The constraint for biases.
use_additive_bias: Whether to use bias while calculating the relevance of inputs features
in additive mode.
use_attention_bias: Whether to use bias while calculating the weights of attention.
attention_activation: The activation used for calculating the weights of attention.
attention_regularizer_weight: The weights of attention regularizer.
kwargs: Parameters for parent class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SeqSelfAttention(tfl.Layer):
    ATTENTION_TYPE_ADD = &#39;additive&#39;
    ATTENTION_TYPE_MUL = &#39;multiplicative&#39;

    def __init__(self,
                 units=32,
                 attention_width=None,
                 attention_type=ATTENTION_TYPE_ADD,
                 return_attention=False,
                 history_only=False,
                 kernel_initializer=&#39;glorot_normal&#39;,
                 bias_initializer=&#39;zeros&#39;,
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 use_additive_bias=True,
                 use_attention_bias=True,
                 attention_activation=None,
                 attention_regularizer_weight=0.0,
                 **kwargs):
        
        &#34;&#34;&#34;Layer initialization. modified from https://github.com/CyberZHG
        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf
        Args
            units: The dimension of the vectors that used to calculate the attention weights.
            attention_width: The width of local attention.
            attention_type: &#39;additive&#39; or &#39;multiplicative&#39;.
            return_attention: Whether to return the attention weights for visualization.
            history_only: Only use historical pieces of data.
            kernel_initializer: The initializer for weight matrices.
            bias_initializer: The initializer for biases.
            kernel_regularizer: The regularization for weight matrices.
            bias_regularizer: The regularization for biases.
            kernel_constraint: The constraint for weight matrices.
            bias_constraint: The constraint for biases.
            use_additive_bias: Whether to use bias while calculating the relevance of inputs features
                                    in additive mode.
            use_attention_bias: Whether to use bias while calculating the weights of attention.
            attention_activation: The activation used for calculating the weights of attention.
            attention_regularizer_weight: The weights of attention regularizer.
            kwargs: Parameters for parent class.
        &#34;&#34;&#34; 

        super().__init__(**kwargs)
        self.supports_masking = True
        self.units = units
        self.attention_width = attention_width
        self.attention_type = attention_type
        self.return_attention = return_attention
        self.history_only = history_only
        if history_only and attention_width is None:
            self.attention_width = int(1e9)

        self.use_additive_bias = use_additive_bias
        self.use_attention_bias = use_attention_bias
        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)
        self.bias_initializer = tf.keras.initializers.get(bias_initializer)
        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)
        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)
        self.bias_constraint = tf.keras.constraints.get(bias_constraint)
        self.attention_activation = tf.keras.activations.get(attention_activation)
        self.attention_regularizer_weight = attention_regularizer_weight
        self._backend = tf.keras.backend.backend()

        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            self.Wx, self.Wt, self.bh = None, None, None
            self.Wa, self.ba = None, None
        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            self.Wa, self.ba = None, None
        else:
            raise NotImplementedError(&#39;No implementation for attention type : &#39; + attention_type)

    def get_config(self):
        config = {
            &#39;units&#39;: self.units,
            &#39;attention_width&#39;: self.attention_width,
            &#39;attention_type&#39;: self.attention_type,
            &#39;return_attention&#39;: self.return_attention,
            &#39;history_only&#39;: self.history_only,
            &#39;use_additive_bias&#39;: self.use_additive_bias,
            &#39;use_attention_bias&#39;: self.use_attention_bias,
            &#39;kernel_initializer&#39;: tf.keras.regularizers.serialize(self.kernel_initializer),
            &#39;bias_initializer&#39;: tf.keras.regularizers.serialize(self.bias_initializer),
            &#39;kernel_regularizer&#39;: tf.keras.regularizers.serialize(self.kernel_regularizer),
            &#39;bias_regularizer&#39;: tf.keras.regularizers.serialize(self.bias_regularizer),
            &#39;kernel_constraint&#39;: tf.keras.constraints.serialize(self.kernel_constraint),
            &#39;bias_constraint&#39;: tf.keras.constraints.serialize(self.bias_constraint),
            &#39;attention_activation&#39;: tf.keras.activations.serialize(self.attention_activation),
            &#39;attention_regularizer_weight&#39;: self.attention_regularizer_weight,
        }
        base_config = super(SeqSelfAttention, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def build(self, input_shape):
        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            self._build_additive_attention(input_shape)
        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            self._build_multiplicative_attention(input_shape)
        super(SeqSelfAttention, self).build(input_shape)

    def _build_additive_attention(self, input_shape):
        feature_dim = int(input_shape[2])

        self.Wt = self.add_weight(shape=(feature_dim, self.units),
                                  name=&#39;{}_Add_Wt&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        self.Wx = self.add_weight(shape=(feature_dim, self.units),
                                  name=&#39;{}_Add_Wx&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_additive_bias:
            self.bh = self.add_weight(shape=(self.units,),
                                      name=&#39;{}_Add_bh&#39;.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

        self.Wa = self.add_weight(shape=(self.units, 1),
                                  name=&#39;{}_Add_Wa&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_attention_bias:
            self.ba = self.add_weight(shape=(1,),
                                      name=&#39;{}_Add_ba&#39;.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

    def _build_multiplicative_attention(self, input_shape):
        feature_dim = int(input_shape[2])

        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),
                                  name=&#39;{}_Mul_Wa&#39;.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_attention_bias:
            self.ba = self.add_weight(shape=(1,),
                                      name=&#39;{}_Mul_ba&#39;.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

    def call(self, inputs, mask=None, **kwargs):
        input_len = K.shape(inputs)[1]

        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            e = self._call_additive_emission(inputs)
        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            e = self._call_multiplicative_emission(inputs)

        if self.attention_activation is not None:
            e = self.attention_activation(e)
        e = K.exp(e - K.max(e, axis=-1, keepdims=True))
        if self.attention_width is not None:
            if self.history_only:
                lower = K.arange(0, input_len) - (self.attention_width - 1)
            else:
                lower = K.arange(0, input_len) - self.attention_width // 2
            lower = K.expand_dims(lower, axis=-1)
            upper = lower + self.attention_width
            indices = K.expand_dims(K.arange(0, input_len), axis=0)
            e = e * K.cast(lower &lt;= indices, K.floatx()) * K.cast(indices &lt; upper, K.floatx())
        if mask is not None:
            mask = K.cast(mask, K.floatx())
            mask = K.expand_dims(mask)
            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))

        # a_{t} = \text{softmax}(e_t)
        s = K.sum(e, axis=-1, keepdims=True)
        a = e / (s + K.epsilon())

        # l_t = \sum_{t&#39;} a_{t, t&#39;} x_{t&#39;}
        v = K.batch_dot(a, inputs)
        if self.attention_regularizer_weight &gt; 0.0:
            self.add_loss(self._attention_regularizer(a))

        if self.return_attention:
            return [v, a]
        return v

    def _call_additive_emission(self, inputs):
        input_shape = K.shape(inputs)
        batch_size = input_shape[0]
        input_len = inputs.get_shape().as_list()[1]

        # h_{t, t&#39;} = \tanh(x_t^T W_t + x_{t&#39;}^T W_x + b_h)
        q = K.expand_dims(K.dot(inputs, self.Wt), 2)
        k = K.expand_dims(K.dot(inputs, self.Wx), 1)
        if self.use_additive_bias:
            h = K.tanh(q + k + self.bh)
        else:
            h = K.tanh(q + k)

        # e_{t, t&#39;} = W_a h_{t, t&#39;} + b_a
        if self.use_attention_bias:
            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))
        else:
            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))
        return e

    def _call_multiplicative_emission(self, inputs):
        # e_{t, t&#39;} = x_t^T W_a x_{t&#39;} + b_a
        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))
        if self.use_attention_bias:
            e += self.ba[0]
        return e

    def compute_output_shape(self, input_shape):
        output_shape = input_shape
        if self.return_attention:
            attention_shape = (input_shape[0], output_shape[1], input_shape[1])
            return [output_shape, attention_shape]
        return output_shape

    def compute_mask(self, inputs, mask=None):
        if self.return_attention:
            return [mask, None]
        return mask

    def _attention_regularizer(self, attention):
        batch_size = K.cast(K.shape(attention)[0], K.floatx())
        input_len = K.shape(attention)[-1]
        indices = K.expand_dims(K.arange(0, input_len), axis=0)
        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)
        eye = K.cast(K.equal(indices, diagonal), K.floatx())
        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(
            attention,
            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size

    @staticmethod
    def get_custom_objects():
        return {&#39;SeqSelfAttention&#39;: SeqSelfAttention}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="nais.Layers.attention.SeqSelfAttention.ATTENTION_TYPE_ADD"><code class="name">var <span class="ident">ATTENTION_TYPE_ADD</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="nais.Layers.attention.SeqSelfAttention.ATTENTION_TYPE_MUL"><code class="name">var <span class="ident">ATTENTION_TYPE_MUL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="nais.Layers.attention.SeqSelfAttention.get_custom_objects"><code class="name flex">
<span>def <span class="ident">get_custom_objects</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_custom_objects():
    return {&#39;SeqSelfAttention&#39;: SeqSelfAttention}</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="nais.Layers.attention.SeqSelfAttention.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Instance of <code>TensorShape</code>, or list of instances of
<code>TensorShape</code> if the layer expects a list of inputs
(one instance per input).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
        self._build_additive_attention(input_shape)
    elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
        self._build_multiplicative_attention(input_shape)
    super(SeqSelfAttention, self).build(input_shape)</code></pre>
</details>
</dd>
<dt id="nais.Layers.attention.SeqSelfAttention.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, mask=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or dict/list/tuple of input tensors.
The first positional <code>inputs</code> argument is subject to special rules:
- <code>inputs</code> must be explicitly passed. A layer cannot have zero
arguments, and <code>inputs</code> cannot be provided via the default value
of a keyword argument.
- NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
- Keras mask metadata is only collected from <code>inputs</code>.
- Layers are built (<code>build(input_shape)</code> method)
using shape info from <code>inputs</code> only.
- <code>input_spec</code> compatibility is only checked against <code>inputs</code>.
- Mixed precision input casting is only applied to <code>inputs</code>.
If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
casting behavior in mixed precision should be handled manually.
- The SavedModel input specification is generated using <code>inputs</code> only.
- Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <code>inputs</code> and not for tensors in
positional and keyword arguments.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. May contain tensors, although
this is not recommended, for the reasons above.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. May contain tensors, although
this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <code>training</code>: Boolean scalar tensor of Python boolean indicating
whether the <code>call</code> is meant for training or inference.
- <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a
<code>mask</code> argument, its default value will be set to the mask generated
for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, mask=None, **kwargs):
    input_len = K.shape(inputs)[1]

    if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
        e = self._call_additive_emission(inputs)
    elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
        e = self._call_multiplicative_emission(inputs)

    if self.attention_activation is not None:
        e = self.attention_activation(e)
    e = K.exp(e - K.max(e, axis=-1, keepdims=True))
    if self.attention_width is not None:
        if self.history_only:
            lower = K.arange(0, input_len) - (self.attention_width - 1)
        else:
            lower = K.arange(0, input_len) - self.attention_width // 2
        lower = K.expand_dims(lower, axis=-1)
        upper = lower + self.attention_width
        indices = K.expand_dims(K.arange(0, input_len), axis=0)
        e = e * K.cast(lower &lt;= indices, K.floatx()) * K.cast(indices &lt; upper, K.floatx())
    if mask is not None:
        mask = K.cast(mask, K.floatx())
        mask = K.expand_dims(mask)
        e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))

    # a_{t} = \text{softmax}(e_t)
    s = K.sum(e, axis=-1, keepdims=True)
    a = e / (s + K.epsilon())

    # l_t = \sum_{t&#39;} a_{t, t&#39;} x_{t&#39;}
    v = K.batch_dot(a, inputs)
    if self.attention_regularizer_weight &gt; 0.0:
        self.add_loss(self._attention_regularizer(a))

    if self.return_attention:
        return [v, a]
    return v</code></pre>
</details>
</dd>
<dt id="nais.Layers.attention.SeqSelfAttention.compute_mask"><code class="name flex">
<span>def <span class="ident">compute_mask</span></span>(<span>self, inputs, mask=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes an output mask tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Tensor or list of tensors.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Tensor or list of tensors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None or a tensor (or list of tensors,
one per output tensor of the layer).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_mask(self, inputs, mask=None):
    if self.return_attention:
        return [mask, None]
    return mask</code></pre>
</details>
</dd>
<dt id="nais.Layers.attention.SeqSelfAttention.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <code>build</code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Shape tuple (tuple of integers)
or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An input shape tuple.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
    output_shape = input_shape
    if self.return_attention:
        attention_shape = (input_shape[0], output_shape[1], input_shape[1])
        return [output_shape, attention_shape]
    return output_shape</code></pre>
</details>
</dd>
<dt id="nais.Layers.attention.SeqSelfAttention.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;units&#39;: self.units,
        &#39;attention_width&#39;: self.attention_width,
        &#39;attention_type&#39;: self.attention_type,
        &#39;return_attention&#39;: self.return_attention,
        &#39;history_only&#39;: self.history_only,
        &#39;use_additive_bias&#39;: self.use_additive_bias,
        &#39;use_attention_bias&#39;: self.use_attention_bias,
        &#39;kernel_initializer&#39;: tf.keras.regularizers.serialize(self.kernel_initializer),
        &#39;bias_initializer&#39;: tf.keras.regularizers.serialize(self.bias_initializer),
        &#39;kernel_regularizer&#39;: tf.keras.regularizers.serialize(self.kernel_regularizer),
        &#39;bias_regularizer&#39;: tf.keras.regularizers.serialize(self.bias_regularizer),
        &#39;kernel_constraint&#39;: tf.keras.constraints.serialize(self.kernel_constraint),
        &#39;bias_constraint&#39;: tf.keras.constraints.serialize(self.bias_constraint),
        &#39;attention_activation&#39;: tf.keras.activations.serialize(self.attention_activation),
        &#39;attention_regularizer_weight&#39;: self.attention_regularizer_weight,
    }
    base_config = super(SeqSelfAttention, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nais.Layers" href="index.html">nais.Layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="nais.Layers.attention.SeqSelfAttention" href="#nais.Layers.attention.SeqSelfAttention">SeqSelfAttention</a></code></h4>
<ul class="">
<li><code><a title="nais.Layers.attention.SeqSelfAttention.ATTENTION_TYPE_ADD" href="#nais.Layers.attention.SeqSelfAttention.ATTENTION_TYPE_ADD">ATTENTION_TYPE_ADD</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.ATTENTION_TYPE_MUL" href="#nais.Layers.attention.SeqSelfAttention.ATTENTION_TYPE_MUL">ATTENTION_TYPE_MUL</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.build" href="#nais.Layers.attention.SeqSelfAttention.build">build</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.call" href="#nais.Layers.attention.SeqSelfAttention.call">call</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.compute_mask" href="#nais.Layers.attention.SeqSelfAttention.compute_mask">compute_mask</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.compute_output_shape" href="#nais.Layers.attention.SeqSelfAttention.compute_output_shape">compute_output_shape</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.get_config" href="#nais.Layers.attention.SeqSelfAttention.get_config">get_config</a></code></li>
<li><code><a title="nais.Layers.attention.SeqSelfAttention.get_custom_objects" href="#nais.Layers.attention.SeqSelfAttention.get_custom_objects">get_custom_objects</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>